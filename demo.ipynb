{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(size, unit=None):\n",
    "    if unit == \"KB\":\n",
    "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
    "    elif unit == \"MB\":\n",
    "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
    "    else:\n",
    "        return print('File size: ' + str(size) + ' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Fashion MNIST dataset\n",
    "\n",
    "Dataset contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fashion MNIST dataset](dataset.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[88], cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.4960 - accuracy: 0.8253\n",
      "Epoch 2/12\n",
      "1875/1875 [==============================] - 2s 888us/step - loss: 0.3700 - accuracy: 0.8678\n",
      "Epoch 3/12\n",
      "1875/1875 [==============================] - 2s 872us/step - loss: 0.3372 - accuracy: 0.8756\n",
      "Epoch 4/12\n",
      "1875/1875 [==============================] - 2s 860us/step - loss: 0.3126 - accuracy: 0.8862\n",
      "Epoch 5/12\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.2953 - accuracy: 0.8907\n",
      "Epoch 6/12\n",
      "1875/1875 [==============================] - 2s 874us/step - loss: 0.2791 - accuracy: 0.8958\n",
      "Epoch 7/12\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.2676 - accuracy: 0.9011\n",
      "Epoch 8/12\n",
      "1875/1875 [==============================] - 2s 893us/step - loss: 0.2573 - accuracy: 0.9043\n",
      "Epoch 9/12\n",
      "1875/1875 [==============================] - 2s 902us/step - loss: 0.2460 - accuracy: 0.9080\n",
      "Epoch 10/12\n",
      "1875/1875 [==============================] - 2s 936us/step - loss: 0.2386 - accuracy: 0.9114\n",
      "Epoch 11/12\n",
      "1875/1875 [==============================] - 2s 888us/step - loss: 0.2295 - accuracy: 0.9148\n",
      "Epoch 12/12\n",
      "1875/1875 [==============================] - 2s 874us/step - loss: 0.2229 - accuracy: 0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c050fd90>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_MODEL_NAME = \"tf_model_fashion_mnist.h5\"\n",
    "model.save(KERAS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 1.191 Megabytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_size = get_file_size(KERAS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.3389 - accuracy: 0.8816 - 276ms/epoch - 881us/step\n",
      "\n",
      "Test accuracy is 88.16%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy is {}%'.format(round(100*test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmpx6q8n7m6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmpx6q8n7m6/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2021-11-09 14:42:27.976261: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-11-09 14:42:27.976278: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2021-11-09 14:42:27.976506: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmpx6q8n7m6\n",
      "2021-11-09 14:42:27.977879: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2021-11-09 14:42:27.977899: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmpx6q8n7m6\n",
      "2021-11-09 14:42:27.983321: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2021-11-09 14:42:28.026436: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmpx6q8n7m6\n",
      "2021-11-09 14:42:28.040768: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 64263 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "408680"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\"\n",
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = tf_lite_converter.convert()\n",
    "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 399.102 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_file_size = get_file_size(TF_LITE_MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check input Tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [ 1 28 28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [ 1 10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [10000    28    28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [10000    10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
    "interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs_numpy = np.array(test_images, dtype=np.float32)\n",
    "test_imgs_numpy.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy)\n",
    "interpreter.invoke()\n",
    "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "prediction_classes = np.argmax(tflite_model_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy TFLITE model is 88.16%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(prediction_classes, test_labels)\n",
    "print('Test accuracy TFLITE model is {}%'.format(round(100*acc, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3273084907352828"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_file_size/keras_model_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â TensorFlow Lite model Float 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp2igi0kqa/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp2igi0kqa/assets\n",
      "2021-11-09 14:42:29.577913: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-11-09 14:42:29.577933: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2021-11-09 14:42:29.578076: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp2igi0kqa\n",
      "2021-11-09 14:42:29.579497: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2021-11-09 14:42:29.579511: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp2igi0kqa\n",
      "2021-11-09 14:42:29.585053: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2021-11-09 14:42:29.632898: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp2igi0kqa\n",
      "2021-11-09 14:42:29.647145: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 69071 microseconds.\n"
     ]
    }
   ],
   "source": [
    "TF_LITE_MODEL_FLOAT_16_FILE_NAME = \"tf_lite_float_16_model.tflite\"\n",
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tf_lite_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205776"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = TF_LITE_MODEL_FLOAT_16_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 200.953 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16480432609754223"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_16_file_size = get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME)\n",
    "tflite_float_16_file_size/keras_model_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035137515904864"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_16_file_size/tflite_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Lite Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_LITE_SIZE_QUANT_MODEL_FILE_NAME = \"tf_lite_quant_model.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp9le8p2ci/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp9le8p2ci/assets\n",
      "2021-11-09 14:43:50.930150: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-11-09 14:43:50.930164: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2021-11-09 14:43:50.930272: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp9le8p2ci\n",
      "2021-11-09 14:43:50.931323: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2021-11-09 14:43:50.931333: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp9le8p2ci\n",
      "2021-11-09 14:43:50.935382: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2021-11-09 14:43:50.976221: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/30/_b_jsy997wl5p8hwvx0dhlpr0000gq/T/tmp9le8p2ci\n",
      "2021-11-09 14:43:50.990425: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 60153 microseconds.\n"
     ]
    }
   ],
   "source": [
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103984"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = TF_LITE_SIZE_QUANT_MODEL_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 101.547 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_float_quant_file_size = get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08327994054178733"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_quant_file_size/keras_model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053261799238007"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_quant_file_size/ tflite_float_16_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [ 1 28 28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [ 1 10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [10000    28    28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [10000    10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
    "interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_numpy = np.array(test_images, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy)\n",
    "interpreter.invoke()\n",
    "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "prediction_classes = np.argmax(tflite_model_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(prediction_classes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy TFLITE Quantized model is 88.21%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy TFLITE Quantized model is {}%'.format(round(100*acc, 2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
